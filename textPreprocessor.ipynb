{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cdbb7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import json\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5498b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the JSON file\n",
    "with open('redditGamingData.json', 'r') as jsonFile:\n",
    "    reddit_data = json.load(jsonFile)\n",
    "    \n",
    "with open('youtubeGamingData.json', 'r') as jsonFile:\n",
    "    youtube_data = json.load(jsonFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3af495d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit:\n",
      "Total number of posts: 833\n",
      "Total number of comments: 25333\n",
      "\n",
      "YouTube:\n",
      "Total number of posts: 70\n",
      "Total number of comments: 3064\n"
     ]
    }
   ],
   "source": [
    "# Print the number of posts and comments in Reddit data\n",
    "print(\"Reddit:\")\n",
    "print(f\"Total number of posts: {len(reddit_data)}\")\n",
    "print(f\"Total number of comments: {sum(len(submission['comments']) for submission in reddit_data)}\")\n",
    "\n",
    "# Print the number of posts and comments in YouTube data\n",
    "print(\"\\nYouTube:\")\n",
    "print(f\"Total number of posts: {len(youtube_data)}\")\n",
    "print(f\"Total number of comments: {sum(len(videos['comments']) for videos in youtube_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c1db9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count the number of words\n",
    "def count_words(data):\n",
    "    # initialise the counts with 0\n",
    "    subWordCount = 0\n",
    "    comWordCount = 0\n",
    "\n",
    "    # Iterate through all submission to count the words in each submission\n",
    "    for submission in data:\n",
    "        \n",
    "        # Count words in the title\n",
    "        subWordCount += len(submission['title'].split())\n",
    "    \n",
    "        # count words in the comments\n",
    "        for comment in submission['comments']:\n",
    "            comWordCount += len(comment.split())\n",
    "\n",
    "    print(f\"Total number of words in posts: {subWordCount}\")\n",
    "    print(f\"Total number of words in comments: {comWordCount}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22ea9a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit:\n",
      "Total number of words in posts: 11800\n",
      "Total number of words in comments: 1290522\n",
      "\n",
      "YouTube:\n",
      "Total number of words in posts: 600\n",
      "Total number of words in comments: 73878\n"
     ]
    }
   ],
   "source": [
    "# Print the number of words in posts and comments in Reddit data before preprocessing\n",
    "print(\"Reddit:\")\n",
    "count_words(reddit_data)\n",
    "\n",
    "# Print the number of words in posts and comments in YouTube data before preprocessing\n",
    "print(\"\\nYouTube:\")\n",
    "count_words(youtube_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e42aeee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to count the number of urls in the data\n",
    "def count_urls(data):\n",
    "    # Regular expression to match URLs\n",
    "    url_pattern = r'http\\S+|www\\S+|https\\S+'\n",
    "    \n",
    "    total_urls = 0\n",
    "\n",
    "    for submission in data:\n",
    "        # Count URLs in the title\n",
    "        total_urls += len(re.findall(url_pattern, submission['title'], flags=re.MULTILINE))\n",
    "    \n",
    "        # Count URLs in the comments\n",
    "        for comment in submission['comments']:\n",
    "            total_urls += len(re.findall(url_pattern, comment, flags=re.MULTILINE))\n",
    "\n",
    "    print(f\"Total number of URLs in the data: {total_urls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b5acbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit:\n",
      "Total number of URLs in the data: 1468\n",
      "\n",
      "YouTube:\n",
      "Total number of URLs in the data: 21\n"
     ]
    }
   ],
   "source": [
    "# Print the number of urls in posts and comments in Reddit data before preprocessing\n",
    "print(\"Reddit:\")\n",
    "count_urls(reddit_data)\n",
    "\n",
    "# Print the number of urls in posts and comments in YouTube data before preprocessing\n",
    "print(\"\\nYouTube:\")\n",
    "count_urls(youtube_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3539ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    # remove non ascii characters\n",
    "    if not text.isascii():\n",
    "        return '' \n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Keep only alphabetic characters and spaces\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Initialize the lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Lemmatize words\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return ' '.join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a2f63f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    # Create an empty list to store clean data\n",
    "    cleanData = []\n",
    "\n",
    "    # iterate through submission to preprocess the data\n",
    "    for submission in data:\n",
    "        # Process title\n",
    "        title = preprocess_text(submission['title'])\n",
    "        \n",
    "        # Process comments\n",
    "        comments = []\n",
    "        for comment in submission['comments']:\n",
    "            comment = preprocess_text(comment)\n",
    "        \n",
    "            # Add only if comment is present\n",
    "            if comment.strip():\n",
    "                comments.append(comment)\n",
    "     \n",
    "        # Add only if title is present\n",
    "        if title.strip():\n",
    "            if 'score' in submission:\n",
    "                submission = {\n",
    "                    'title': title,\n",
    "                    'date': submission['date'],\n",
    "                    'ID' : submission['ID'],\n",
    "                    'keyword': submission['keyword'],\n",
    "                    'score': submission['score'],\n",
    "                    'comments': comments\n",
    "                }\n",
    "            else:\n",
    "                submission = {\n",
    "                    'title': title,\n",
    "                    'date': submission['date'],\n",
    "                    'ID' : submission['ID'],\n",
    "                    'keyword': submission['keyword'],\n",
    "                    'comments': comments\n",
    "                }\n",
    "        cleanData.append(submission)\n",
    "    return cleanData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1579770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Reddit data\n",
    "cleanRedditData = clean_data(reddit_data)\n",
    "\n",
    "# Preprocess YouTube data\n",
    "cleanYTData = clean_data(youtube_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a892bcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit:\n",
      "Total number of posts: 833\n",
      "Total number of comments: 19619\n",
      "\n",
      "YouTube:\n",
      "Total number of posts: 70\n",
      "Total number of comments: 2481\n"
     ]
    }
   ],
   "source": [
    "# Print the number of posts and comments in Reddit data after preprocessing\n",
    "print(\"Reddit:\")\n",
    "print(f\"Total number of posts: {len(cleanRedditData)}\")\n",
    "print(f\"Total number of comments: {sum(len(submission['comments']) for submission in cleanRedditData)}\")\n",
    "\n",
    "# Print the number of posts and comments in YouTube data after preprocessing\n",
    "print(\"\\nYouTube:\")\n",
    "print(f\"Total number of posts: {len(cleanYTData)}\")\n",
    "print(f\"Total number of comments: {sum(len(videos['comments']) for videos in cleanYTData)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36400e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit:\n",
      "Total number of words in posts: 7536\n",
      "Total number of words in comments: 568778\n",
      "\n",
      "YouTube:\n",
      "Total number of words in posts: 470\n",
      "Total number of words in comments: 32444\n"
     ]
    }
   ],
   "source": [
    "# Print the number of words in posts and comments in Reddit data after preprocessing\n",
    "print(\"Reddit:\")\n",
    "count_words(cleanRedditData)\n",
    "\n",
    "# Print the number of words in posts and comments in YouTube data after preprocessing\n",
    "print(\"\\nYouTube:\")\n",
    "count_words(cleanYTData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "800089ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit:\n",
      "Total number of URLs in the data: 159\n",
      "\n",
      "YouTube:\n",
      "Total number of URLs in the data: 2\n"
     ]
    }
   ],
   "source": [
    "# Print the number of urls in posts and comments in Reddit data before preprocessing\n",
    "print(\"Reddit:\")\n",
    "count_urls(cleanRedditData)\n",
    "\n",
    "# Print the number of urls in posts and comments in YouTube data before preprocessing\n",
    "print(\"\\nYouTube:\")\n",
    "count_urls(cleanYTData)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
