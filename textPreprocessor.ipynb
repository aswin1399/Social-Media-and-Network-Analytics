{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bac266f8",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "\n",
    "This script contains preprocessing of text in the Reddit and YouTube data that we collected and stored in `json` file. The main goals is to clean the data, remove unwanted elements such as URLs and mentions. Below is the step-by-step breakdown of the process.\n",
    "\n",
    "\n",
    "### Import Libraries\n",
    "\n",
    "Import necessary libraries for text processing and regular expression operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cdbb7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import json\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45debd9",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "\n",
    "Load that data that we collected from Reddit and YouTube from the `json` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5498b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the JSON file\n",
    "with open('redditGamingData.json', 'r') as jsonFile:\n",
    "    redditData = json.load(jsonFile)\n",
    "    \n",
    "with open('youtubeGamingData.json', 'r') as jsonFile:\n",
    "    youtubeData = json.load(jsonFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df7f4d2",
   "metadata": {},
   "source": [
    "### Data Overview\n",
    "\n",
    "Let's check the basic informations about the data such as number of posts and comments, number of words, number of urls and number of mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3af495d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit:\n",
      "Total number of posts: 1269\n",
      "Total number of comments: 44401\n",
      "\n",
      "YouTube:\n",
      "Total number of posts: 109\n",
      "Total number of comments: 6900\n"
     ]
    }
   ],
   "source": [
    "# Print the number of posts and comments in Reddit data\n",
    "print(\"Reddit:\")\n",
    "print(f\"Total number of posts: {len(redditData)}\")\n",
    "print(f\"Total number of comments: {sum(len(submission['comments']) for submission in redditData)}\")\n",
    "\n",
    "# Print the number of posts and comments in YouTube data\n",
    "print(\"\\nYouTube:\")\n",
    "print(f\"Total number of posts: {len(youtubeData)}\")\n",
    "print(f\"Total number of comments: {sum(len(videos['comments']) for videos in youtubeData)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa9d048",
   "metadata": {},
   "source": [
    "Define a function `count_words` to count the number of words in the title and comments seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c1db9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count the number of words\n",
    "def count_words(data):\n",
    "    \n",
    "    # initialise the counts with 0\n",
    "    subWordCount = 0\n",
    "    comWordCount = 0\n",
    "\n",
    "    # Iterate through all submission to count the words in each submission\n",
    "    for submission in data:\n",
    "        \n",
    "        # Count words in the title\n",
    "        subWordCount += len(submission['title'].split())\n",
    "    \n",
    "        # count words in the comments\n",
    "        for comment in submission['comments']:\n",
    "            comWordCount += len(comment['comment_body'].split())\n",
    "\n",
    "    print(f\"Total number of words in posts: {subWordCount}\")\n",
    "    print(f\"Total number of words in comments: {comWordCount}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22ea9a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit:\n",
      "Total number of words in posts: 19306\n",
      "Total number of words in comments: 1963998\n",
      "\n",
      "YouTube:\n",
      "Total number of words in posts: 1035\n",
      "Total number of words in comments: 124575\n"
     ]
    }
   ],
   "source": [
    "# Print the number of words in posts and comments in Reddit data before preprocessing\n",
    "print(\"Reddit:\")\n",
    "count_words(redditData)\n",
    "\n",
    "# Print the number of words in posts and comments in YouTube data before preprocessing\n",
    "print(\"\\nYouTube:\")\n",
    "count_words(youtubeData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f685013",
   "metadata": {},
   "source": [
    "Define a function `count_url` to count the number of urls present in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e42aeee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to count the number of urls in the data\n",
    "def count_urls(data):\n",
    "    # Regular expression to match URLs\n",
    "    url_pattern = r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+|www\\.(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n",
    "    \n",
    "    total_urls = 0\n",
    "\n",
    "    for submission in data:\n",
    "        # Count URLs in the title\n",
    "        total_urls += len(re.findall(url_pattern, submission['title'], flags=re.MULTILINE))\n",
    "    \n",
    "        # Count URLs in the comments\n",
    "        for comment in submission['comments']:\n",
    "            total_urls += len(re.findall(url_pattern, comment['comment_body'], flags=re.MULTILINE))\n",
    "\n",
    "    print(f\"Total number of URLs in the data: {total_urls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b5acbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit:\n",
      "Total number of URLs in the data: 2259\n",
      "\n",
      "YouTube:\n",
      "Total number of URLs in the data: 21\n"
     ]
    }
   ],
   "source": [
    "# Print the number of urls in posts and comments in Reddit data before preprocessing\n",
    "print(\"Reddit:\")\n",
    "count_urls(redditData)\n",
    "\n",
    "# Print the number of urls in posts and comments in YouTube data before preprocessing\n",
    "print(\"\\nYouTube:\")\n",
    "count_urls(youtubeData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe69a80c",
   "metadata": {},
   "source": [
    "The above results indicate that the data contains several URLs which need to be removed.\n",
    "\n",
    "Define a function `check_mentions` to check the mentions(for example: @username) in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "080866c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_mentions(data):\n",
    "    pattern = r'@\\w+'  # Pattern to match mentions like @username\n",
    "    mentions = []  # List to store all found mentions\n",
    "    \n",
    "    for submission in data:\n",
    "        # Check for mentions in the title\n",
    "        title_mentions = re.findall(pattern, submission['title'])\n",
    "        if title_mentions:\n",
    "            mentions.extend(title_mentions)\n",
    "            print(f\"Mentions found in post ID {submission['ID']} (Title): {title_mentions}\")\n",
    "        \n",
    "        # Check for mentions in the comments\n",
    "        for comment in submission['comments']:\n",
    "            comment_mentions = re.findall(pattern, comment['comment_body'])\n",
    "            if comment_mentions:\n",
    "                mentions.extend(comment_mentions)\n",
    "                print(f\"Mentions found in post ID {submission['ID']} (Comment Author: {comment['comment_author']}): {comment_mentions}\")\n",
    "    \n",
    "    print(f\"\\nTotal number of mentions found: {len(mentions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6234c861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for mentions in Reddit data:\n",
      "Mentions found in post ID 48 (Comment Author: PandaCheese2016): ['@Civilization']\n",
      "Mentions found in post ID 100 (Comment Author: AutoModerator): ['@openai']\n",
      "Mentions found in post ID 116 (Comment Author: wanderingnexus): ['@ss']\n",
      "Mentions found in post ID 161 (Comment Author: AutoModerator): ['@openai']\n",
      "Mentions found in post ID 185 (Comment Author: ChocolateAxis): ['@zigzagame']\n",
      "Mentions found in post ID 219 (Comment Author: AutoModerator): ['@openai']\n",
      "Mentions found in post ID 348 (Comment Author: PCMRBot): ['@home']\n",
      "Mentions found in post ID 377 (Comment Author: PrepperLady999): ['@ExpertlyAmateur']\n",
      "Mentions found in post ID 382 (Comment Author: Affectionate_West725): ['@Tammy']\n",
      "Mentions found in post ID 653 (Comment Author: TELETUBB13S): ['@plhought']\n",
      "Mentions found in post ID 697 (Comment Author: Intrepid-Extent6611): ['@r']\n",
      "Mentions found in post ID 775 (Comment Author: AutoModerator): ['@hiraedu']\n",
      "Mentions found in post ID 799 (Comment Author: AutoModerator): ['@hiraedu']\n",
      "Mentions found in post ID 806 (Comment Author: Anonymous): ['@4']\n",
      "Mentions found in post ID 910 (Comment Author: WestQ): ['@OP']\n",
      "Mentions found in post ID 946 (Comment Author: Henno_Tv): ['@h']\n",
      "Mentions found in post ID 1003 (Comment Author: Crazy_Dig_211): ['@lakerfan']\n",
      "Mentions found in post ID 1015 (Comment Author: Anonymous): ['@nts']\n",
      "Mentions found in post ID 1021 (Comment Author: DyingOnHills): ['@51bworld', '@51bworld']\n",
      "Mentions found in post ID 1068 (Title): ['@TheAthletic', '@Stadium']\n",
      "Mentions found in post ID 1072 (Comment Author: SharpCoderC): ['@DevKxm', '@DevKxm']\n",
      "Mentions found in post ID 1085 (Comment Author: Asumed): ['@chest']\n",
      "Mentions found in post ID 1096 (Comment Author: RedneckId1ot): ['@ss']\n",
      "Mentions found in post ID 1129 (Title): ['@FNBRNewsJP']\n",
      "Mentions found in post ID 1134 (Comment Author: -Smaug): ['@Ole', '@Mizzou', '@LSU']\n",
      "Mentions found in post ID 1157 (Comment Author: devilmasterrace): ['@gmail', '@gmail']\n",
      "Mentions found in post ID 1192 (Comment Author: richarde_2001): ['@Equilant_Kiwi']\n",
      "Mentions found in post ID 1213 (Title): ['@TickPick']\n",
      "Mentions found in post ID 1223 (Title): ['@FNBRIntel']\n",
      "Mentions found in post ID 1252 (Comment Author: SharpCoderC): ['@DevKxm', '@DevKxm']\n",
      "\n",
      "Total number of mentions found: 37\n",
      "\n",
      "Checking for mentions in YouTube data:\n",
      "Mentions found in post ID 1 (Comment Author: @rk-jn5mp): ['@0']\n",
      "Mentions found in post ID 7 (Comment Author: @aidannortham9842): ['@Greenskull']\n",
      "Mentions found in post ID 9 (Comment Author: @JSK010): ['@3']\n",
      "Mentions found in post ID 22 (Comment Author: @GreenskullAI): ['@Greenskull']\n",
      "Mentions found in post ID 22 (Comment Author: @snakeyash007): ['@kryptoniteworld']\n",
      "Mentions found in post ID 50 (Comment Author: @notrelikk): ['@joshseki']\n",
      "Mentions found in post ID 61 (Comment Author: @kronho25): ['@gameranx']\n",
      "Mentions found in post ID 62 (Comment Author: @Assmask): ['@3']\n",
      "Mentions found in post ID 72 (Comment Author: @gamemaster27): ['@1']\n",
      "Mentions found in post ID 81 (Comment Author: @michaelc754): ['@8']\n",
      "Mentions found in post ID 81 (Comment Author: @YonkoSlimeball4L): ['@gameranx']\n",
      "Mentions found in post ID 90 (Comment Author: @00maljaso): ['@2']\n",
      "Mentions found in post ID 92 (Comment Author: @greengrugach1984): ['@05']\n",
      "Mentions found in post ID 94 (Comment Author: @norseman325): ['@GamerZakh']\n",
      "Mentions found in post ID 102 (Comment Author: @Hydestown): ['@JakeBaldino']\n",
      "Mentions found in post ID 107 (Comment Author: @hannahrowefitness): ['@Ozblox5']\n",
      "\n",
      "Total number of mentions found: 16\n"
     ]
    }
   ],
   "source": [
    "# Check for mentions in Reddit data\n",
    "print(\"Checking for mentions in Reddit data:\")\n",
    "check_mentions(redditData)\n",
    "\n",
    "# Check for mentions in YouTube data\n",
    "print(\"\\nChecking for mentions in YouTube data:\")\n",
    "check_mentions(youtubeData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed425322",
   "metadata": {},
   "source": [
    "From the above overview we have found that our data contains URLs and mention which are irrelavent for our analysis and are to be removed.\n",
    "\n",
    "\n",
    "### Extracting Hashtags\n",
    "\n",
    "The data contains hashtags(#) which are used for several reasons in Reddit and YouTube such as increasing the reach of audience, categorizing content, etc. Let's extract these hashtags and check the top 10 popular hastags. \n",
    "\n",
    "For that a fucntion `extract_hashtags` is created that check for these hashtags and gives us the top 'n' popular hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d0b3309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hashtags(data):\n",
    "    hashtags = []\n",
    "    hashtag_pattern = r'#\\b[a-zA-Z]+\\b'\n",
    "    \n",
    "    for submission in data:\n",
    "        hashtags += re.findall(hashtag_pattern, submission['title'])\n",
    "        for comment in submission['comments']:\n",
    "            hashtags += re.findall(hashtag_pattern, comment['comment_body'])\n",
    "    \n",
    "    return Counter(hashtags).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e5ee49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 hashtags in Reddit data:\n",
      "Hashtag                       Count\n",
      "--------------------------------------------------\n",
      "#Do                           140\n",
      "#Don                          10\n",
      "#Subreddit                    10\n",
      "#If                           3\n",
      "#MOAR                         3\n",
      "#advanced                     2\n",
      "#mods                         2\n",
      "#button                       2\n",
      "#cyberbullying                2\n",
      "#survey                       2\n",
      "\n",
      "Top 10 hashtags in YouTube data:\n",
      "Hashtag                       Count\n",
      "--------------------------------------------------\n",
      "#gaming                       9\n",
      "#shorts                       6\n",
      "#vr                           3\n",
      "#clips                        2\n",
      "#minecraft                    2\n",
      "#overwatch                    2\n",
      "#games                        2\n",
      "#gameplay                     2\n",
      "#funny                        2\n",
      "#gameshorts                   2\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 10 hashtags in Reddit data:\")\n",
    "print(f\"{'Hashtag':<30}{'Count'}\")\n",
    "print(\"-\" * 50)\n",
    "for hashtag, count in extract_hashtags(redditData):\n",
    "    print(f\"{hashtag:<30}{count}\")\n",
    "\n",
    "print(\"\\nTop 10 hashtags in YouTube data:\")\n",
    "print(f\"{'Hashtag':<30}{'Count'}\")\n",
    "print(\"-\" * 50)\n",
    "for hashtag, count in extract_hashtags(youtubeData):\n",
    "    print(f\"{hashtag:<30}{count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb090e3",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "This step includes:\n",
    "* Convert text to lowercase.\n",
    "* Remove URLs.\n",
    "* Remove mentions .\n",
    "* Remove non-alphabetic characters.\n",
    "* Remove stopwords.\n",
    "* Lemmatize words to their base form.\n",
    "\n",
    "We create a function `preprocess_text` to carry out this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3539ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+|www\\.(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove mentions (e.g., @username)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Keep only alphabetic characters and spaces\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Initialize the lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Lemmatize words\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return ' '.join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da8c6b8",
   "metadata": {},
   "source": [
    "Define a function `clean_data` that will go through each titles and comments of a post and preprocess them using `preprocess_text` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a2f63f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    # Create an empty list to store clean data\n",
    "    cleanData = []\n",
    "\n",
    "    # iterate through submission to preprocess the data\n",
    "    for submission in data:\n",
    "        # Process title\n",
    "        title = preprocess_text(submission['title'])\n",
    "        \n",
    "        # Process comments\n",
    "        comments = []\n",
    "        for comment in submission['comments']:\n",
    "            comment['comment_body'] = preprocess_text(comment['comment_body'])\n",
    "        \n",
    "            # Add only if comment is present\n",
    "            if comment['comment_body'].strip():\n",
    "                comments.append(comment)\n",
    "     \n",
    "        # Add only if title is present\n",
    "        if title.strip():\n",
    "            if 'score' in submission:\n",
    "                submission = {\n",
    "                    'title': title,\n",
    "                    'date': submission['date'],\n",
    "                    'ID' : submission['ID'],\n",
    "                    'keyword': submission['keyword'],\n",
    "                    'score': submission['score'],\n",
    "                    'comments': comments\n",
    "                }\n",
    "            else:\n",
    "                submission = {\n",
    "                    'title': title,\n",
    "                    'date': submission['date'],\n",
    "                    'ID' : submission['ID'],\n",
    "                    'keyword': submission['keyword'],\n",
    "                    'comments': comments\n",
    "                }\n",
    "        cleanData.append(submission)\n",
    "    return cleanData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1579770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Reddit data\n",
    "cleanRedditData = clean_data(redditData)\n",
    "\n",
    "# Preprocess YouTube data\n",
    "cleanYTData = clean_data(youtubeData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240073d3",
   "metadata": {},
   "source": [
    "Now let's check the basic infromation of the data after preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a892bcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit:\n",
      "Total number of posts: 1269\n",
      "Total number of comments: 44272\n",
      "\n",
      "YouTube:\n",
      "Total number of posts: 109\n",
      "Total number of comments: 6768\n"
     ]
    }
   ],
   "source": [
    "# Print the number of posts and comments in Reddit data after preprocessing\n",
    "print(\"Reddit:\")\n",
    "print(f\"Total number of posts: {len(cleanRedditData)}\")\n",
    "print(f\"Total number of comments: {sum(len(submission['comments']) for submission in cleanRedditData)}\")\n",
    "\n",
    "# Print the number of posts and comments in YouTube data after preprocessing\n",
    "print(\"\\nYouTube:\")\n",
    "print(f\"Total number of posts: {len(cleanYTData)}\")\n",
    "print(f\"Total number of comments: {sum(len(videos['comments']) for videos in cleanYTData)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485d7601",
   "metadata": {},
   "source": [
    "It can be observed that the number of comments in the data have been reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36400e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit:\n",
      "Total number of words in posts: 10852\n",
      "Total number of words in comments: 999306\n",
      "\n",
      "YouTube:\n",
      "Total number of words in posts: 691\n",
      "Total number of words in comments: 68480\n"
     ]
    }
   ],
   "source": [
    "# Print the number of words in posts and comments in Reddit data after preprocessing\n",
    "print(\"Reddit:\")\n",
    "count_words(cleanRedditData)\n",
    "\n",
    "# Print the number of words in posts and comments in YouTube data after preprocessing\n",
    "print(\"\\nYouTube:\")\n",
    "count_words(cleanYTData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc26501f",
   "metadata": {},
   "source": [
    "As the preprocessing step will remove the irrelavent items from a text data, the number of words in both titles and comments are decreased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "800089ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit:\n",
      "Total number of URLs in the data: 0\n",
      "\n",
      "YouTube:\n",
      "Total number of URLs in the data: 0\n"
     ]
    }
   ],
   "source": [
    "# Print the number of urls in posts and comments in Reddit data before preprocessing\n",
    "print(\"Reddit:\")\n",
    "count_urls(cleanRedditData)\n",
    "\n",
    "# Print the number of urls in posts and comments in YouTube data before preprocessing\n",
    "print(\"\\nYouTube:\")\n",
    "count_urls(cleanYTData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0c23868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for mentions in Reddit data:\n",
      "\n",
      "Total number of mentions found: 0\n",
      "\n",
      "Checking for mentions in YouTube data:\n",
      "\n",
      "Total number of mentions found: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for mentions in Reddit data\n",
    "print(\"Checking for mentions in Reddit data:\")\n",
    "check_mentions(cleanRedditData)\n",
    "\n",
    "# Check for mentions in YouTube data\n",
    "print(\"\\nChecking for mentions in YouTube data:\")\n",
    "check_mentions(cleanYTData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f3a27e",
   "metadata": {},
   "source": [
    "We can also observe that the mentions and URLs are completely removed from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df8f130",
   "metadata": {},
   "source": [
    "### Save The Data\n",
    "Next we save the collected data as json file which will be used for preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58855f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit and YouTube data saved!!!\n"
     ]
    }
   ],
   "source": [
    "# Save Reddit data to JSON\n",
    "with open('preprocessedRedditData.json', 'w') as jsonFile:\n",
    "    json.dump(redditData, jsonFile, indent=4)\n",
    "\n",
    "# Save YouTube data to JSON\n",
    "with open('preprocessedYoutubeData.json', 'w') as jsonFile:\n",
    "    json.dump(youtubeData, jsonFile, indent=4)\n",
    "\n",
    "print(f\"Reddit and YouTube data saved!!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
