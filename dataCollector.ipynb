{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c45a015a",
   "metadata": {},
   "source": [
    "### Reddit and YouTube Data Collection Script\n",
    "\n",
    "In this script, data collection is performed. The data from Reddit and YouTube based on a set of keywords related to gaming culture (e.g., AI in games, mental health in gaming, etc.) is collected that includes both posts/videos and their respective comments. The collected data is then saved to JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3425adf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "import praw\n",
    "from datetime import datetime\n",
    "from redditClient import redditClient\n",
    "from youtubeClient import youtubeClient\n",
    "from googleapiclient.errors import HttpError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75482c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise clients\n",
    "redditClient = redditClient()\n",
    "youtubeClient = youtubeClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25cdc131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the keywords and time period\n",
    "keywords = ['AI in games', 'mental health in gaming', 'cyberbullying in gaming', 'stress relief in gaming', 'toxicity in gaming', 'upcoming games']\n",
    "year = 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8324e94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise lists to store Reddit and YouTube data\n",
    "redditData = []\n",
    "youtubeData = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dba72f",
   "metadata": {},
   "source": [
    "### Reddit Data Collection\n",
    "\n",
    "The data from Reddit is collected using `search` method of the Reddit API using the specified keywords in the `keywords` list from all subreddits. We are only extracting post title, author, date, score, and up to 50 comments (along with comment authors) and only of the year 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9bf7d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initilise ID as 0\n",
    "ID = 0  \n",
    "\n",
    "# Iterate through the keywords and collect relevant Reddit posts and comments\n",
    "for keyword in keywords:\n",
    "    \n",
    "    # Search posts in 'all' subreddit based on the keywords \n",
    "    for submission in redditClient.subreddit('all').search(keyword, time_filter='year', limit=1000):\n",
    "        \n",
    "        # Ensure the data is from 2024\n",
    "        if datetime.utcfromtimestamp(submission.created_utc).year == year:\n",
    "            \n",
    "            # Get author of the submission (Assign Anonymous if not present)\n",
    "            sub_author = submission.author.name if submission.author else 'Anonymous'\n",
    "            \n",
    "            # Extracting comments with their authors\n",
    "            comments = []\n",
    "            for comment in submission.comments[:50]:\n",
    "                if isinstance(comment, praw.models.Comment):\n",
    "                    \n",
    "                    # Get author of the comment (Assign Anonymous if not present)\n",
    "                    comment_author = comment.author.name if comment.author else 'Anonymous'\n",
    "                    comments.append({\n",
    "                        'comment_body': comment.body,\n",
    "                        'comment_author': comment_author\n",
    "                    })\n",
    "            \n",
    "            # Add submission data into a dictionary\n",
    "            subData = {\n",
    "                'title': submission.title,\n",
    "                'author': sub_author,\n",
    "                'ID': ID,\n",
    "                'date': datetime.fromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'keyword': keyword,\n",
    "                'score': submission.score,\n",
    "                'comments': comments\n",
    "            }\n",
    "            ID += 1\n",
    "            \n",
    "            # Store the data\n",
    "            redditData.append(subData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bc80c2",
   "metadata": {},
   "source": [
    "Now print the total number of posts and comments that we collected from Reddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6df06fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of posts: 1269\n",
      "Total number of comments: 44401\n"
     ]
    }
   ],
   "source": [
    "# Print the number of posts\n",
    "print(f\"Total number of posts: {len(redditData)}\")\n",
    "\n",
    "# Calculate the total number of comments\n",
    "print(f\"Total number of comments: {sum(len(submission['comments']) for submission in redditData)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45af566c",
   "metadata": {},
   "source": [
    "### YouTube Data Collection\n",
    "\n",
    "Similar to Reddit data collection, the data from YouTube is collected using `search` method of the YouTube API using the specified keywords in the `keywords` list. Here, we are only extracting video title, author, date, and up to 50 comments (along with comment authors) and only of the year 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e0a112c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comments are disabled for video 0dEm2lF2dH4. Skipping...\n"
     ]
    }
   ],
   "source": [
    "# Initilise ID as 0\n",
    "ID = 0  \n",
    "\n",
    "# Iterate through the keywords and collect YouTube videos and their comments\n",
    "for keyword in keywords:\n",
    "    \n",
    "    # Search YouTube videos based on the keyword\n",
    "    videos = youtubeClient.search().list(q=keyword, part='snippet', type='video', maxResults=1000).execute()\n",
    "\n",
    "    for video in videos['items']:\n",
    "        video_id = video['id']['videoId']\n",
    "        title = video['snippet']['title']\n",
    "        video_author = video['snippet']['channelTitle']  # Get the uploader (channel name)\n",
    "        published_at = video['snippet']['publishedAt'][:10]  # Get date in YYYY-MM-DD format\n",
    "        y = int(published_at.split('-')[0])\n",
    "        \n",
    "        # Filter by the current year (2024)\n",
    "        if y == year:\n",
    "            try:\n",
    "                comments_response = youtubeClient.commentThreads().list(part='snippet', videoId=video_id, maxResults=1000).execute()\n",
    "                \n",
    "                # Extract comments along with their authors\n",
    "                comments = []\n",
    "                for comment in comments_response['items']:\n",
    "                    comment_text = comment['snippet']['topLevelComment']['snippet']['textOriginal']\n",
    "                    comment_author = comment['snippet']['topLevelComment']['snippet']['authorDisplayName']  # Get comment author name\n",
    "                    comments.append({\n",
    "                        'comment_body': comment_text,\n",
    "                        'comment_author': comment_author\n",
    "                    })\n",
    "                \n",
    "                # Add the video data into a dictionary \n",
    "                video_data = {\n",
    "                    'title': title,\n",
    "                    'author': video_author,  # The uploader of the video (channel name)\n",
    "                    'ID': ID,\n",
    "                    'date': published_at,\n",
    "                    'keyword': keyword,\n",
    "                    'comments': comments\n",
    "                }\n",
    "            except HttpError as e:\n",
    "                # Handle the case where comments are disabled\n",
    "                print(f\"Comments are disabled for video {video_id}. Skipping...\")\n",
    "                continue\n",
    "            ID += 1\n",
    "            \n",
    "            # Store the data\n",
    "            youtubeData.append(video_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dfc1ac",
   "metadata": {},
   "source": [
    "Now display the number of video data collected and number of comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c40f861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of posts: 109\n",
      "Total number of comments: 6900\n"
     ]
    }
   ],
   "source": [
    "# Print the number of posts\n",
    "print(f\"Total number of posts: {len(youtubeData)}\")\n",
    "\n",
    "# Calculate the total number of comments\n",
    "print(f\"Total number of comments: {sum(len(videos['comments']) for videos in youtubeData)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75566bdc",
   "metadata": {},
   "source": [
    "### Save The Data\n",
    "\n",
    "Next we save the collected data as `json` file which will be used for preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a30f0e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit and YouTube data saved!!!\n"
     ]
    }
   ],
   "source": [
    "# Save Reddit data to JSON\n",
    "with open('redditGamingData.json', 'w') as jsonFile:\n",
    "    json.dump(redditData, jsonFile, indent=4)\n",
    "\n",
    "# Save YouTube data to JSON\n",
    "with open('youtubeGamingData.json', 'w') as jsonFile:\n",
    "    json.dump(youtubeData, jsonFile, indent=4)\n",
    "\n",
    "print(f\"Reddit and YouTube data saved!!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
